## 2015-11-11 大更新哦

完全可以跑了。我才发现因为知乎可以设置未登陆用户不允许查看数据。所以。。。哈哈

现在成功啦~

## 2015-11-11 夜

上次成功的跑出了200万的索引数据。非常的开心。

然后号就被封了。。。

后来测试获取详细数据的时候，总是会出错。

查出了问题，知乎会对首次访问某用户的时候给一个不同的className。导致正则出错。

然后写了一个更加健壮的正则来做。

本地测试效果很棒，没有任何问题。然而放到服务器上就出现了这样那样的错误。主要就是还是正则匹配那里的问题。我现在怎么都排不出错了。因为在本地开发机上是好好的。
我拿出错的用户数据去做正则匹配也都是正确的。

只能说见鬼了。

还把swoole下了。可能服务器性能太差。现在不怎么敢上多线程了。

等下一个项目。要从一开始就上最大的规模，直接走多线程。之后的修改代码就不会像现在这么痛苦了。

至少是能跑了。正好明天课多。跑一天好了。明天看看效果.

## 2015-11-08 夜

swoole已经配置上了。明天详细测试

阿里云正在跑这个程序。好棒~

## 2015-10-07 晚上

---

整套流程已经可以跑了，没有任何问题。问题只是性能不好。

我惊奇的发现通过linux命令行似乎不会崩溃。挺好的。

正在写`curl_muti`这一块儿的内容，说实话，文档很难看懂。

明天再弄吧。

`swoole`也准备开始了！

## 2015-10-07 下午

---

Subject: 添加宕机恢复

每次输入数据库之后就会把当前的次数和id写入cache文件。

一旦崩溃重启应用，会从cache文件中拿出数据，接着运行。

嘿嘿，这使得写循环脚本成为了可能哦~

不过，切换两个任务要把cache给删掉。不然会出错的。

嗯，可以写个命令行脚本啦


## 2015-10-07

------

Subject: 正常运行

今天已经可以正常运行了。一个是用户表的获取，另一个是详细信息的获取。

问题也是有的

比如最迫切的应该是日志系统，不能老是出了错没地方找吧。

第二个就是排重的问题。目前为止并没有加入任何去重机制，所以数据库别看有几万条数据了，实际上排重下来也没多少。

针对这个问题，我想到的方法就是每次完成了一个用户的读取，就把这个用户id存本地文件里。就算挂了，重启服务重新从本地文件读取id，然后继续走。

然后是性能问题。

上了`php7`以后，实测在`php-cli`下，对用户详细信息的抓取大概到170条左右就挂了。

这个很明显不行啊！目标是一百万的数据量。怎么能才100+的数据就挂掉？

还需要一些用户界面，这个倒是其次的，后面再讲啦。

好了，暂时如此


